{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsZalQwFRNc3MKSMmBwsnY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TruongNVMM/Machine-Learning/blob/main/Email_Classification_Vector_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlZNZHiLKIAv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq faiss-cpu\n",
        "!pip install -qq transformers\n",
        "!pip install -qq tqdm"
      ],
      "metadata": {
        "id": "K1YgwGigKev8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "slH4JOk5NgS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Data Science Projects/Machine Learning Projects/Email_classification/2cls_spam_text_cls.csv\"\n",
        "data = pd.read_csv(data_path)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "Ql8ISek6OC6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = data[\"Message\"].values.tolist()\n",
        "labels = data[\"Category\"].values.tolist()"
      ],
      "metadata": {
        "id": "oyfH7NwzOcNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Name = \"intfloat/multilingual-e5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(Model_Name)\n",
        "model = AutoModel.from_pretrained(Model_Name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)/content/drive/MyDrive/Data Science Projects/Machine Learning Projects/Email_classification/\n",
        "model.eval()\n",
        "\n",
        "def average_pool(last_hidden_states, attention_mask):\n",
        "  last_hidden = last_hidden_states.masked_fill(\n",
        "      ~attention_mask[..., None].bool(), 0.0\n",
        "  )\n",
        "  return last_hidden.sum(dim=1)/attention_mask.sum(dim=1)[..., None]"
      ],
      "metadata": {
        "id": "ky6bt989Ove7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
        "  embeddings = []\n",
        "  for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    batch_texts_with_prefix = [f\"passage: {text}\" for text in batch_texts]\n",
        "\n",
        "    batch_dict = tokenizer(batch_texts_with_prefix, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "    batch_dict = {k: v.to(device) for k,v, in batch_dict.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**batch_dict)\n",
        "      batch_embeddings = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
        "      batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "      embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "  return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(labels)\n",
        "\n",
        "X_embeddings = get_embeddings(messages, model, tokenizer, device)\n",
        "\n",
        "metadata = [{\"index\":i, \"message\":message, \"label\":label, \"label_encoded\":y[i]}\n",
        "            for i, (message, label) in enumerate(zip(messages, labels))]"
      ],
      "metadata": {
        "id": "M8uFIxh9QIAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata"
      ],
      "metadata": {
        "id": "FByDm2e-fRp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_Size = 0.1\n",
        "SEED = 42\n",
        "\n",
        "train_indices, test_indices = train_test_split(range(len(messages)), test_size=Test_Size, stratify=y, random_state=SEED)\n",
        "X_train_emb = X_embeddings[train_indices]\n",
        "X_test_emb = X_embeddings[test_indices]\n",
        "\n",
        "train_metadata = [metadata[i] for i in train_indices]\n",
        "test_metadata = [metadata[i] for i in test_indices]\n",
        "\n",
        "embedding_dim = X_train_emb.shape[1]\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "index.add(X_train_emb.astype(\"float32\"))"
      ],
      "metadata": {
        "id": "odWJquOlVDXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metadata"
      ],
      "metadata": {
        "id": "sxepCTTM7kT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_knn(query_text, model, tokenizer, device, index, train_metadata, k=1):\n",
        "  query_with_predix = f\"query: {query_text}\"\n",
        "  batch_dict = tokenizer([query_with_predix], max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "  batch_dict = {k: v.to(device) for k,v in batch_dict.items()}\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**batch_dict)\n",
        "    query_embedding = average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
        "    query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
        "    query_embedding = query_embedding.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "  scores, indices = index.search(query_embedding, k)\n",
        "\n",
        "  predictions = []\n",
        "  neighbor_info = []\n",
        "\n",
        "  for i in range(k):\n",
        "    neighbor_idx = indices[0][i]\n",
        "    neighbor_score = scores[0][i]\n",
        "    neighbor_label = train_metadata[neighbor_idx][\"label\"]\n",
        "    neighbor_message = train_metadata[neighbor_idx][\"message\"]\n",
        "\n",
        "    predictions.append(neighbor_label)\n",
        "    neighbor_info.append(\n",
        "        {\n",
        "            \"score\": float(neighbor_score),\n",
        "            \"label\": neighbor_label,\n",
        "            \"message\": neighbor_message[:100] + \"...\" if len(neighbor_message) > 100 else neighbor_message\n",
        "        }\n",
        "    )\n",
        "\n",
        "    unique_labels, counts = np.unique(predictions, return_counts=True)\n",
        "    final_prediction = unique_labels[np.argmax(counts)]\n",
        "\n",
        "    return final_prediction, neighbor_info"
      ],
      "metadata": {
        "id": "R3YWLJR6f6Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_knn_accuracy(test_embeddings, test_metadata, index, train_metadata, k_values=[1, 3, 5]):\n",
        "  results = {}\n",
        "  all_errors = {}\n",
        "\n",
        "  for k in k_values:\n",
        "    correct = 0\n",
        "    total = len(test_embeddings)\n",
        "    errors = []\n",
        "\n",
        "    for i in tqdm(range(total), desc=f\"Evaluating k={k}\"):\n",
        "      query_embedding = test_embeddings[i:i+1].astype(\"float32\")\n",
        "      true_label = test_metadata[i][\"label\"]\n",
        "      true_message = test_metadata[i][\"message\"]\n",
        "\n",
        "      scores, indices = index.search(query_embedding, k)\n",
        "\n",
        "      predictions = []\n",
        "      neighbor_details = []\n",
        "\n",
        "      for j in range(k):\n",
        "        neighbor_idx = indices[0][j]\n",
        "        neighbor_label = train_metadata[neighbor_idx][\"label\"]\n",
        "        neighbor_message = train_metadata[neighbor_idx][\"message\"]\n",
        "        neighbor_score = float(scores[0][j])\n",
        "\n",
        "        predictions.append(neighbor_label)\n",
        "        neighbor_details.append(\n",
        "            {\n",
        "                \"label\": neighbor_label,\n",
        "                \"message\": neighbor_message,\n",
        "                \"score\": neighbor_score\n",
        "            }\n",
        "        )\n",
        "\n",
        "        unique_labels, counts = np.unique(predictions, return_counts=True)\n",
        "        predicted_label = unique_labels[np.argmax(counts)]\n",
        "\n",
        "        if predicted_label == true_label:\n",
        "          correct += 1\n",
        "        else:\n",
        "          error_info = {\n",
        "              \"index\": i,\n",
        "              \"original_index\": test_metadata[i][\"index\"],\n",
        "              \"message\": true_message,\n",
        "              \"predicted_label\": predicted_label,\n",
        "              \"neighbors\": neighbor_details,\n",
        "              \"label_distribution\": {label: int(count) for label, count in zip(unique_labels, counts)}\n",
        "          }\n",
        "\n",
        "          errors.append(error_info)\n",
        "    accuracy = correct / total\n",
        "    error_count = total - correct\n",
        "\n",
        "    results[k] = accuracy\n",
        "    all_errors[k] = errors\n",
        "\n",
        "    print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n",
        "    print(f\"Number of errors with k={k}: {error_count}/{total} ({(error_count/total)*100:.2f}%)\")\n",
        "\n",
        "  return results, all_errors"
      ],
      "metadata": {
        "id": "k03hU_hKjhE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Evaluating accuracy on test set...\")\n",
        "accuracy_results, error_results = evaluate_knn_accuracy(X_test_emb, test_metadata, index, train_metadata, k_values=[1, 3, 5])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Accuracy Results\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for k, accuracy in accuracy_results.items():\n",
        "  print(f\"Top-{k} accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "error_analysis = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"model\": Model_Name,\n",
        "    \"test_size\": len(X_test_emb),\n",
        "    \"accuracy_results\": accuracy_results,\n",
        "    \"errors_by_k\": {}\n",
        "}\n",
        "\n",
        "for k,errors in error_results.items():\n",
        "  error_analysis[\"errors_by_k\"][f\"k_{k}\"] = {\n",
        "      \"total_errors\": len(errors),\n",
        "      \"error_rate\": len(errors)/len(X_test_emb),\n",
        "      \"errors\": errors\n",
        "  }\n",
        "\n",
        "output_file = \"/content/drive/MyDrive/Data Science Projects/Machine Learning Projects/Email_classification/error_analysis.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(error_analysis, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n***Error analysis saved to: {output_file}***\")\n",
        "print()\n",
        "print(f\"***Summary:\")\n",
        "for k, errors in error_results.items():\n",
        "  print(f\" k={k}: {len(errors)} errors out of {len(X_test_emb)} samples\")"
      ],
      "metadata": {
        "id": "F3iuhB400y0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9H3JirE7Opv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}